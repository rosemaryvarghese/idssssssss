{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cifar10 Classification: Vision Transformer\n","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing necessary dependencies\nimport numpy as np\nimport pandas as pd\nimport sklearn.metrics\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.preprocessing\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, Dropout, AlphaDropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.metrics import recall_score, classification_report, auc, roc_curve\nfrom sklearn.metrics import precision_recall_fscore_support, f1_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom numpy import dstack\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    input_shape = (7,7,1)\n    learning_rate = 0.001\n    weight_decay = 0.0001\n    batch_size = 256\n    num_classes = 10\n    num_epochs = 100\n    image_size = 72\n    patch_size = 6\n    num_patches = (image_size // patch_size) ** 2\n    projection_dim = 64\n    num_heads = 4\n    transformer_units = [\n        projection_dim * 2,\n        projection_dim\n    ]\n    transformer_layers = 8\n    mlp_head_units = [2048, 1024]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"markdown","source":"### Sample Images","metadata":{}},{"cell_type":"code","source":"d1 = pd.read_csv('../input/beepsandbops/UNSW_NB15_training-set.csv')\nt1 = pd.read_csv('../input/beepsandbops/UNSW_NB15_testing-set.csv')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import and preprcess dataset","metadata":{}},{"cell_type":"code","source":"data = d1.append(t1, ignore_index = True)\nfrom sklearn import preprocessing\ndata.drop(columns = 'label', inplace = True)\nmultic = pd.DataFrame(data.attack_cat)\nle2 = preprocessing.LabelEncoder()\nenc_label = multic.apply(le2.fit_transform)\ndata['attax'] = enc_label\ncategorical_features = ['proto', 'service', 'state']\nlabel_feature = ['attack_cat', 'attax']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_features = ['id', 'sttl', 'dttl', 'swin', 'dwin', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']\nnumerical_features = list(set(data.columns) - set(label_feature) - set(categorical_features) - set(drop_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\ndata.drop(columns = categorical_features, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dropna(inplace=True)\ndata.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = data['attax']\nX = data.drop(columns = ['attax', 'attack_cat'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tab2img\nfrom tab2img.converter import Tab2Img\nscaler = preprocessing.MinMaxScaler().fit(X)\nX_scaled_train = scaler.transform(X)\ny_scaler =  preprocessing.StandardScaler().fit(Y.values.reshape(-1, 1))\ny_scaled_train = y_scaler.transform(Y.values.reshape(-1, 1))\nmodel = Tab2Img()\ntrain_images = model.fit_transform(X_scaled_train, y_scaled_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Model, optimizers\nfrom tensorflow.keras import Sequential\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, Input, Dropout, Activation, Dense, MaxPooling2D, Flatten, GlobalAveragePooling2D\nimport sklearn\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_shape = (train_images.shape[0], train_images.shape[1], train_images.shape[2], 1)\nY = tf.keras.utils.to_categorical(Y)\ntrain_images = train_images.reshape(new_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, test_data, train_labels, test_labels = train_test_split(train_images, Y, random_state=10, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sample_images(images, row_count, column_count):\n    fig, axs = plt.subplots(row_count, column_count, figsize=(10,10))\n    for i in range(row_count):\n        for j in range(column_count):\n            axs[i,j].imshow(images[i * column_count + j])\n            axs[i,j].axis('off')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = np.random.choice(train_data.shape[0], 100)\nsample_images(train_data[indices], 10, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:34:43.93151Z","iopub.execute_input":"2021-10-06T17:34:43.931777Z","iopub.status.idle":"2021-10-06T17:34:43.939137Z","shell.execute_reply.started":"2021-10-06T17:34:43.931749Z","shell.execute_reply":"2021-10-06T17:34:43.938461Z"}}},{"cell_type":"code","source":"augmentation_layer = tf.keras.Sequential([\n    keras.layers.Input(Config.input_shape),\n    keras.layers.experimental.preprocessing.Normalization(),\n    keras.layers.experimental.preprocessing.Resizing(Config.image_size, Config.image_size),\n    keras.layers.experimental.preprocessing.RandomRotation(factor=0.02),\n    keras.layers.experimental.preprocessing.RandomZoom(height_factor=0.2, width_factor=0.2),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmentation_layer.layers[0].adapt(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MLP","metadata":{}},{"cell_type":"code","source":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Patch Creation Layer","metadata":{}},{"cell_type":"code","source":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n        \n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images = images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(4, 4))\nimage = train_data[np.random.choice(range(train_data.shape[0]))]\nprint(image.shape)\nplt.imshow(np.squeeze(image).astype(\"uint8\"))\nplt.axis(\"off\")\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(Config.image_size, Config.image_size)\n)\nprint(resized_image.shape)\npatches = Patches(Config.patch_size)(resized_image)\nprint(f\"Image size: {Config.image_size} X {Config.image_size}\")\nprint(f\"Patch size: {Config.patch_size} X {Config.patch_size}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Patch Encoder Layer","metadata":{}},{"cell_type":"code","source":"class PatchEncoder(layers.Layer):\n    \n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the Model","metadata":{}},{"cell_type":"code","source":"def create_vision_transformer():\n    # Inputs\n    inputs = layers.Input(shape=Config.input_shape)\n    # Data Augmentation\n    augmented = augmentation_layer(inputs)\n    # Patches\n    patches = Patches(Config.patch_size)(augmented)\n    encoder_patches = PatchEncoder(Config.num_patches, Config.projection_dim)(patches)\n    \n    for _ in range(Config.transformer_layers):\n        # Layer Normalization 1\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoder_patches)\n        # Multi-Head Attention Layer\n        attention_output = layers.MultiHeadAttention(\n            num_heads=Config.num_heads, \n            key_dim=Config.projection_dim,\n            dropout=0.2\n        )(x1, x1)\n        # Skip Connnection 1\n        x2 = layers.Add()([attention_output, encoder_patches])\n        \n        # Layer Normalization 2\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        \n        # MLP\n        x3 = mlp(x3, hidden_units=Config.transformer_units, dropout_rate=0.2)\n        \n        # Skip Connnection 2\n        encoder_patches = layers.Add()([x3, x2])\n    \n    representation = layers.LayerNormalization(epsilon=1e-6)(encoder_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    \n    features = mlp(representation, hidden_units=Config.mlp_head_units, dropout_rate=0.5)\n    \n    outputs = layers.Dense(Config.num_classes)(features)\n    \n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nvit_classifier = create_vision_transformer()\nvit_classifier.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"optimizer = tfa.optimizers.AdamW(\n    learning_rate=Config.learning_rate,\n    weight_decay=Config.weight_decay\n)\nvit_classifier.compile(\n    loss=keras.losses.CategoricalCrossentropy(),\n    optimizer=optimizer,\n    metrics=[\"accuracy\"]\n)\ncheckpoint_path = \"model.h5\"\ncheckpoint = keras.callbacks.ModelCheckpoint(\n    checkpoint_path,\n    monitor=\"val_accuracy\",\n    save_best_only=True,\n    save_weights_only=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = vit_classifier.fit(train_data, train_labels, epochs=30, batch_size= 500, validation_split = 0.2, callbacks=[checkpoint])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_results1 = vit_classifier.evaluate(test_data, test_labels, verbose=1)\nprint(f'Test results1 -  Accuracy: {test_results1[1]*100}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}